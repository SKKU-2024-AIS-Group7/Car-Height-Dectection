{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/vscode/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-5-22 Python-3.11.9 torch-2.2.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolov5_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "mask_rcnn_model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "mask_rcnn_model.eval()\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image file not found at {image_path}\")\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image, image_rgb\n",
    "\n",
    "def detect_objects(image):\n",
    "    results = yolov5_model(image)\n",
    "    return results.xyxy[0].cpu().numpy()\n",
    "\n",
    "def load_coco_annotations(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    return annotations\n",
    "\n",
    "def segment_objects(image, boxes, target_size=(224, 224)):\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize(target_size, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "    masks = []\n",
    "\n",
    "    for _, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box[:4])\n",
    "        cropped_image = image_tensor[:, y1:y2, x1:x2]\n",
    "        resized_image = transform(cropped_image.permute(1, 2, 0).numpy())\n",
    "        resized_image = resized_image.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = mask_rcnn_model(resized_image)\n",
    "        mask = output[0]['masks'][0, 0].mul(255).byte().cpu().numpy()\n",
    "        masks.append((x1, y1, x2, y2, mask))\n",
    "\n",
    "    return masks\n",
    "\n",
    "def extract_vehicle_heights(annotations, category_id):\n",
    "    data = []\n",
    "\n",
    "    for ann in annotations['annotations']:\n",
    "        if ann['category_id'] == category_id:\n",
    "            image_id = ann['image_id']\n",
    "            bbox = ann['bbox']\n",
    "            height_real = ann.get('height', None)\n",
    "            if height_real is not None:\n",
    "                data.append({\n",
    "                    'image_id': image_id,\n",
    "                    'bbox': bbox,\n",
    "                    'height_real': height_real\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def calculate_pixel_heights(image_path, vehicle_data):\n",
    "    heights = []\n",
    "\n",
    "    for _, row in vehicle_data.iterrows():\n",
    "        image, image_rgb = load_image(image_path)\n",
    "        x1, y1, x2, y2 = map(int, row['bbox'])\n",
    "        box = np.array([[x1, y1, x2, y2]])\n",
    "        masks = segment_objects(image_rgb, box)\n",
    "        mask_info = masks[0]\n",
    "        x1, y1, x2, y2, mask = mask_info\n",
    "        vehicle_height_pixels = y2 - y1\n",
    "        heights.append({\n",
    "            'image_id': row['image_id'],\n",
    "            'height_pixels': vehicle_height_pixels,\n",
    "            'height_real': row['height']\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(heights)\n",
    "\n",
    "def train_correction_factor_model(vehicle_data):\n",
    "    vehicle_data['correction_factor'] = vehicle_data['height_real'] / vehicle_data['height_pixels']\n",
    "\n",
    "    X = vehicle_data[['height_pixels']]\n",
    "    y = vehicle_data['correction_factor']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def draw_boxes_and_masks(image, boxes, masks, model):\n",
    "    heights = []\n",
    "\n",
    "    for _, (_, mask_info) in enumerate(zip(boxes, masks)):\n",
    "        x1, y1, x2, y2, mask = mask_info\n",
    "\n",
    "        colored_mask = np.zeros_like(image, dtype=np.uint8)\n",
    "        colored_mask[y1:y2, x1:x2][mask > 127] = (0, 255, 0)\n",
    "\n",
    "        image = cv2.addWeighted(image, 1, colored_mask, 0.5, 0)\n",
    "\n",
    "        vehicle_height_pixels = y2 - y1\n",
    "\n",
    "        correction_factor = model.predict([[vehicle_height_pixels]])[0]\n",
    "        vehicle_height_real = vehicle_height_pixels * correction_factor\n",
    "        heights.append(vehicle_height_real)\n",
    "\n",
    "    return image, heights\n",
    "\n",
    "def main(image_path, annotation_file, vehicle_category_id):\n",
    "    annotations = load_coco_annotations(annotation_file)\n",
    "    vehicle_data = extract_vehicle_heights(annotations, vehicle_category_id)\n",
    "    pixel_height_data = calculate_pixel_heights(image_path, vehicle_data)\n",
    "    model = train_correction_factor_model(pixel_height_data)\n",
    "    \n",
    "    image, image_rgb = load_image(image_path)\n",
    "    boxes = detect_objects(image_rgb)\n",
    "    vehicle_boxes = [box for box in boxes if int(box[5]) in [2, 5, 7]]\n",
    "    masks = segment_objects(image_rgb, vehicle_boxes)\n",
    "    image_with_masks, heights = draw_boxes_and_masks(image_rgb, vehicle_boxes, masks, model)\n",
    "\n",
    "    output_image = cv2.cvtColor(image_with_masks, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite('output_image_with_masks.jpg', output_image)\n",
    "\n",
    "    return output_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
